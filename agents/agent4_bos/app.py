import os
from typing import Dict, Any
from fastapi import FastAPI, HTTPException
from langchain_community.chat_models import ChatOllama
from pydantic import BaseModel
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Ollama Draft Generator API",
    description="API for generating draft responses using Ollama LLM",
    version="1.0.0"
)

# Initialize the LLM with better configuration
try:
    llm = ChatOllama(
        model=os.getenv("OLLAMA_MODEL", "llama3"),
        base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
        temperature=0.7,
        timeout=60  # Add timeout to prevent hanging requests
    )
    logger.info(f"LLM initialized with model: {os.getenv('OLLAMA_MODEL', 'llama3')}")
except Exception as e:
    logger.error(f"Failed to initialize LLM: {e}")
    llm = None

# Environment variables
COLLECTION = os.getenv("COLLECTION", "agent4_bos")

# Pydantic models for request/response validation
class RunRequest(BaseModel):
    input: str
    max_length: int = 500  # Optional parameter with default
    
class RunResponse(BaseModel):
    draft: str
    collection: str
    status: str = "success"

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "draft-generator",
        "collection": COLLECTION,
        "llm_available": llm is not None
    }

@app.get("/health")
async def health_check():
    """Detailed health check"""
    if llm is None:
        raise HTTPException(status_code=503, detail="LLM not initialized")
    
    try:
        # Simple test to verify Ollama is responsive
        test_response = llm.invoke("Say 'OK'")
        llm_status = "connected"
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        llm_status = f"disconnected: {str(e)}"
    
    return {
        "status": "healthy" if llm_status == "connected" else "degraded",
        "llm": llm_status,
        "collection": COLLECTION,
        "model": os.getenv("OLLAMA_MODEL", "llama3")
    }

@app.post("/run", response_model=RunResponse)
async def run(request: RunRequest):
    """
    Generate a draft response or summary for the given input.
    
    Args:
        request: Contains the input text and optional parameters
        
    Returns:
        Draft response generated by the LLM
    """
    if llm is None:
        raise HTTPException(status_code=503, detail="LLM service not available")
    
    if not request.input or request.input.strip() == "":
        raise HTTPException(status_code=400, detail="Input cannot be empty")
    
    try:
        logger.info(f"Generating draft for input (length: {len(request.input)})")
        
        # Create a more robust prompt
        prompt = f"""
        Please generate a concise draft response or summary for the following:
        
        {request.input}
        
        Requirements:
        - Be clear and professional
        - Keep it under {request.max_length} words
        - Focus on key points
        - Use neutral tone
        
        Draft:
        """
        
        response = llm.invoke(prompt)
        
        logger.info(f"Draft generated successfully (length: {len(response.content)})")
        
        return RunResponse(
            draft=response.content.strip(),
            collection=COLLECTION
        )
        
    except Exception as e:
        logger.error(f"Error generating draft: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Failed to generate draft: {str(e)}"
        )

# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "detail": exc.detail,
            "status": "error",
            "collection": COLLECTION
        }
    )

if __name__ == "__main__":
    import uvicorn
    
    # Get port from environment variable, default to 8000
    port = int(os.getenv("PORT", 8000))
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        reload=os.getenv("ENVIRONMENT", "development") == "development",
        log_level="info"
    )